26
As our computational power increases, 
the number of problems that we can
seriously consider solving also increases

climate modeling: more accurate models
protein folding: study molecules such as proteins is limited
by computational power
drug: computational analysis of genomes 基因组
energy research
data analysis: majority of data is useless unless analyzed
huge quantities of data are generated by particle colliders






27
increasing density of transistors 晶体管 (electronic switches on
integrated circuits) cause in increase in single-processor performance

size of transistor decrease, speed increase,
overall speed of integrated circuit can be increased

as speed of transistors increases, power consumption increases
power dissipated as heat

when integrated circuit gets too hot, unreliable
impossible to increase speed of integrated circuits

moral imperative 道义上必须做的事 to increase computational power
build more powerful computer: parallelism

rather than monolithic 单块集成电路,单片电路; 整体的;巨石的,庞大的 processors,
put multiple processors on a single chip

multicore processors
core = cpu
conventional processor with 1 cpu = single-core system

don't want to run multiple instances of a game
want program to run faster with more realistic graphics

to make use of multiple cores
1. rewrite serial 顺序的 依次的 连续的 programs to be parallel
2. write translation programs (convert serial into parallel programs)
(little success)

an efficient parallel implementation of a serial program may not be obtained by
finding efficient parallelizations of each of its steps. 
rather, the best parallelization may be obtained by 
devising an entirely new algorithm





28
//serial
sum = 0;
for(i = 0; i < n; i++)
{
	x = compute_next(...);
	sum += x;
}


/* p cores, p <= n, each core can form a partial sum of n / p values 
my_ indicates each core is using its own private variables 私有变量 */
my_sum = 0
my_first_i = ...;
my_last_i = ...;
for(my_i = my_first_i; my_i < my_last_i; my_i++)
{
	my_x = compute_next(...);
	my_sum += my_x;
}



/* when cores are done computing my_sum
can form a global sum by sending results to "master" core */
if(i am master core)
{
	sum = my_sum;
	for each core other than myself
	{
		receive value from core;
		sum += value;
	}
}
else
{
	send my_sum to master
}






29
Instead of making the master core do all the work of computing the
final sum, we can pair the cores so that while core 0 adds in the result of core 1,
core 2 can add in the result of core 3
Then we can repeat the process with only the even-ranked cores: 0 adds in the result
of 2, 4 adds in the result of 6, and so on
(like merge sort)

length of time it takes the program to complete the final sum should
be the length of time it takes for the master to complete


global sum with 8 cores:
method 1: master carry out n - 1 = 8 - 1 = 7 receives and 7 adds
method 2: #parallel additions = depth of tree = i
n / 2^i = 1;	n = 2^i;		log_2 n = i
#parallel additions = #parallel receives = log_2 n = log_2 8 = 3





30
method 1: divide work of adding among cores, repeats basic
serial addition. if p cores, add p values
method 2: bears little relation to the original serial addition
it is unlikely that a translation program would "discover" the
second global sum

partition the work to be done among cores

2 approaches:
1. task-parallelism: 
	partition tasks carried out among cores
2. data-parallelism:
	partition data among cores


grade exam:
100 students, 1 professor, 4 teaching assistants (ta), 5 questions
 
method 1: professor and ta each grade only 1 question of all 100 student responses
(task parallelism, 5 questions = 5 tasks)
method 2: divide 100 exams into 5 piles of 20 exams each
(data parallelism, data = students' papers)


global sum method 1: 
	data-parallelism: data = values computed by compute_next()
	task-parallelism: receiving & adding cores' partial sums

3 types of coordination:
1. communication: more cores send their current partial sums to another core
2. load balancing 负载均衡: amount of time taken by each core to be roughly same
3. sychronization 同步: (conditional wait)
/* ex: read from stdin
if(i am master core)
	for(my_i = 0; my_i < n; my_i++)
		scanf("%lf", &x[my_i]);
要是接收的信息还没到就不能开始求和
we don't want the other cores to race ahead
and start computing their partial sums before 
master is done initializing x and making it available to other cores
cores need to wait before execute below

for(my_i = my_first_i; my_i < my_last_i; my_i++)
	my_sum += x[my_i];

each core will wait in function synchronize_cores() 
until master core has entered the function
*/
 

32
most powerful programs are written
using explicit parallel constructs
